{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15b2d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.90.tar.gz (63.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.90-cp310-cp310-linux_x86_64.whl size=3168735 sha256=cc6ca75a2fa0068ba6dd6bff9fba630d45c6522ea5797dd0daa59aeeba96382c\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/3d/67/02/f950031435db4a5a02e6269f6adb6703bf1631c3616380f3c6\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: triton, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, diskcache, nvidia-cusparse-cu12, nvidia-cudnn-cu12, llama-cpp-python, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch\n",
      "Successfully installed diskcache-5.6.3 huggingface-hub-0.24.6 llama-cpp-python-0.2.90 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 safetensors-0.4.5 tokenizers-0.19.1 torch-2.4.1 transformers-4.44.2 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e1db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c9e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1890b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\" \n",
    "filename = \"mistral-7b-instruct-v0.2.Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf203551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735b17cf841548cba84f336041578776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-7b-instruct-v0.2.Q8_0.gguf:   0%|          | 0.00/7.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded to: /home/ec2-user/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "\n",
    "print(f\"File downloaded to: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b0ddf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/ec2-user/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7338.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 64000\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  8000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 8000.00 MiB, K (f16): 4000.00 MiB, V (f16): 4000.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  4157.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=file_path,\n",
    "    gpu_layers=-1,\n",
    "#     device=\"cuda:0\",\n",
    "    n_ctx=64000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c931832c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1605 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    4819.63 ms\n",
      "llama_print_timings:      sample time =       3.08 ms /    72 runs   (    0.04 ms per token, 23407.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    5127.77 ms /    72 runs   (   71.22 ms per token,    14.04 tokens per second)\n",
      "llama_print_timings:       total time =    5249.16 ms /    72 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.256446838378906\n",
      "<|system|>\n",
      "You are a lawyer chatbot.</s>\n",
      "<|user|>\n",
      " Please reply only Yes or no if the query: What are the reasoning give by kunal saha to prove AMRI hospital guilty ? can be answered as per the context (7, '\n",
      "Death of the claimantâ€™s wife due to cumulative effect of negligence :\n",
      "35. This Court vide its judgment in Malay Kumar Gangulyâ€™s case (supra) has held that:\n",
      "â€œ186. A patient would feel the deficiency in service having regard to the cumulative\n",
      "effect of negligence of all concerned. Negligence on the part of each of the treating\n",
      "doctors as also the hospital may have been the contributing factors to the ultimate\n",
      "death of the patient. But, then in a case of this nature, the court must deal with the\n",
      "consequences the patient faced, keeping in view the cumulative effect. In the instant\n",
      "case, negligent action has been noticed with respect to more than one respondent. A\n",
      "cumulative incidence, therefore, has led to the death of the patient.â€ The two words\n",
      "â€œmayâ€ and â€œcumulative incidenceâ€ in the abovesaid observations of this Court is\n",
      "relevant for determining the quantification of compensation. It is submitted that this\n",
      "Court is also not sure that the negligence solely has contributed to the death of the\n",
      "claimantâ€™s wife. At the most, this Court is of the view that the negligence may have\n",
      "contributed to the death of the claimantâ€™s wife. The incidences leading to or\n",
      "contributing to the death of the deceased are:\n",
      "i) Disease TEN itself is a fatal disease which has very high mortality rate.\n",
      "ii) TEN itself produces septicemic shock and deceased Anuradha died because of such\n",
      "consequence.\n",
      "Indian Kanoon - http://indiankanoon.org/doc/35346928/ 13\n",
      "\n",
      "Page 14 Text:\n",
      "Balram Prasad vs Kunal Saha & Ors on 24 October, 2013\n",
      "iii) No direct treatment or treatment protocol for TEN.\n",
      "iv) Negligence of many in treating deceased Anuradha.\n",
      "v) Contributory negligence on the part of Dr.Kunal Saha and his brother.\n",
      "Furthermore, it is observed factually that lethal combination of Cisapride and Fluconazole had been\n",
      "used for a number of days at Breach Candy Hospital during her stay which leads to cardiac arrest.\n",
      "Therefore, the National Commission ought to have considered different incidences as aforesaid\n",
      "leading to the death of the claimantâ€™s wife so as to correctly apportion the individual liability of the\n",
      "doctors and the AMRI Hospital in causing the death of the wife of the claimant.\n",
      "36. Further, with regard to the liability of each of the doctors and the AMRI Hospital, individual\n",
      "submissions have been made which are presented hereunder:\n",
      "37. It is the case of the appellant-AMRI Hospital that the National Commission should have taken\n",
      "note of the fact that the deceased was initially examined by Dr. Sukumar Mukherjee and the alleged\n",
      "medical negligence resulting in the death of the deceased was due to his wrong medication\n",
      "(overdose of steroid). Therefore, the Hospital has little or minimal responsibility in this regard,\n",
      "particularly, when after admission of the deceased in the Hospital there was correct diagnosis and\n",
      "she was given best possible treatment. The National Commission erred in apportioning the liability\n",
      "on the Hospital to the extent of 25% of the total award. This Court in the earlier round of litigation\n",
      "held that there is no medical negligence by Dr. Kaushik Nandy, the original respondent No.6 in the\n",
      "complaint, who was also a doctor in the appellant-Hospital.\n",
      "38. Further, the learned senior counsel for the AMRI Hospital submitted that the arguments\n",
      "advanced on behalf of the appellants- doctors Dr. Balram Prasad in C.A. No.2867/2012, Dr.\n",
      "Sukumar Mukherjee in C.A. No.858/2012 and Dr. Baidyanath Haldar in C.A. 731/2012 with regard\n",
      "to percentage, on the basis of costs imposed in paragraph 196 of the judgment in the earlier round of\n",
      "litigation is without any basis and further submitted that under the heading â€“ â€˜Individual Liability of\n",
      "Doctorsâ€™ findings as to what was the negligence of the doctors and the appellant AMRI Hospital is\n",
      "not stated. If the said findings of the National Commission are considered, then it cannot be argued\n",
      "that the appellant AMRI Hospital should pay the highest compensation. Further, the learned senior\n",
      "counsel rebutted the submission of the claimant contending that since he had himself claimed\n",
      "special damages against the appellant-doctors, the Hospital and Dr. Abani Roy Choudhary in the\n",
      "complaint before the National Commission, therefore, he cannot now contend contrary to the same\n",
      "in the appeal before this Court.\n",
      "CIVIL APPEAL NO. 858 OF 2012\n",
      "39. It is the case of the appellant- Dr. Sukumar Mukherjee that the National Commission while\n",
      "apportioning the liability of the appellant, has wrongly observed that :\n",
      "Indian Kanoon - http://indiankanoon.org/doc/35346928/ 14\n",
      "\n",
      "Page 15 Text:\n",
      "Balram Prasad vs Kunal Saha & Ors on 24 October, 2013\n",
      "â€œSupreme Court has primarily found Dr.Sukumar Mukherjee and AMRI hospital\n",
      "guilty of negligence and deficient in service on several counts. Therefore, going by the\n",
      "said findings and observations of Supreme Court we consider it appropriate to\n",
      "apportion the liability of Dr. Sukumar Mukherjee and AMRI hospital in equal\n",
      "proportion, i.e. each should pay 25% i.e. 38,90,000/- of the awarded amount of\n",
      "1,55,60,000/-.â€\n",
      "40. It is submitted by the learned counsel for the appellant - Dr. Sukumar Mukherjee that scrutiny\n",
      "of the judgment in Malay Kumar Gangulyâ€™s case (supra) will show that at no place did the Honâ€™ble\n",
      "Supreme Court made any observation or recorded any finding that the appellant Dr. Mukherjee and\n",
      "the Hospital are primarily responsible.Kunal Kaha provided 3 concrete proves to the court which were enough to prove AMRI hospital guilty')</s>\n",
      "<|assistant|>\n",
      "No, the text provided does not explicitly state that Kunal Saha provided three concrete proves to the court to prove AMRI hospital guilty. However, it does mention that he argued for the cumulative effect of negligence from multiple parties, including the hospital, and that the National Commission should have considered all contributing factors when apportioning liability.\n",
      "**************************************************\n",
      "Tokens: 975\n",
      " No, the text provided does not explicitly state that Kunal Saha provided three concrete proves to the court to prove AMRI hospital guilty. However, it does mention that he argued for the cumulative effect of negligence from multiple parties, including the hospital, and that the National Commission should have considered all contributing factors when apportioning liability.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"(7, '\\nDeath of the claimantâ€™s wife due to cumulative effect of negligence :\\n35. This Court vide its judgment in Malay Kumar Gangulyâ€™s case (supra) has held that:\\nâ€œ186. A patient would feel the deficiency in service having regard to the cumulative\\neffect of negligence of all concerned. Negligence on the part of each of the treating\\ndoctors as also the hospital may have been the contributing factors to the ultimate\\ndeath of the patient. But, then in a case of this nature, the court must deal with the\\nconsequences the patient faced, keeping in view the cumulative effect. In the instant\\ncase, negligent action has been noticed with respect to more than one respondent. A\\ncumulative incidence, therefore, has led to the death of the patient.â€ The two words\\nâ€œmayâ€ and â€œcumulative incidenceâ€ in the abovesaid observations of this Court is\\nrelevant for determining the quantification of compensation. It is submitted that this\\nCourt is also not sure that the negligence solely has contributed to the death of the\\nclaimantâ€™s wife. At the most, this Court is of the view that the negligence may have\\ncontributed to the death of the claimantâ€™s wife. The incidences leading to or\\ncontributing to the death of the deceased are:\\ni) Disease TEN itself is a fatal disease which has very high mortality rate.\\nii) TEN itself produces septicemic shock and deceased Anuradha died because of such\\nconsequence.\\nIndian Kanoon - http://indiankanoon.org/doc/35346928/ 13\\n\\nPage 14 Text:\\nBalram Prasad vs Kunal Saha & Ors on 24 October, 2013\\niii) No direct treatment or treatment protocol for TEN.\\niv) Negligence of many in treating deceased Anuradha.\\nv) Contributory negligence on the part of Dr.Kunal Saha and his brother.\\nFurthermore, it is observed factually that lethal combination of Cisapride and Fluconazole had been\\nused for a number of days at Breach Candy Hospital during her stay which leads to cardiac arrest.\\nTherefore, the National Commission ought to have considered different incidences as aforesaid\\nleading to the death of the claimantâ€™s wife so as to correctly apportion the individual liability of the\\ndoctors and the AMRI Hospital in causing the death of the wife of the claimant.\\n36. Further, with regard to the liability of each of the doctors and the AMRI Hospital, individual\\nsubmissions have been made which are presented hereunder:\\n37. It is the case of the appellant-AMRI Hospital that the National Commission should have taken\\nnote of the fact that the deceased was initially examined by Dr. Sukumar Mukherjee and the alleged\\nmedical negligence resulting in the death of the deceased was due to his wrong medication\\n(overdose of steroid). Therefore, the Hospital has little or minimal responsibility in this regard,\\nparticularly, when after admission of the deceased in the Hospital there was correct diagnosis and\\nshe was given best possible treatment. The National Commission erred in apportioning the liability\\non the Hospital to the extent of 25% of the total award. This Court in the earlier round of litigation\\nheld that there is no medical negligence by Dr. Kaushik Nandy, the original respondent No.6 in the\\ncomplaint, who was also a doctor in the appellant-Hospital.\\n38. Further, the learned senior counsel for the AMRI Hospital submitted that the arguments\\nadvanced on behalf of the appellants- doctors Dr. Balram Prasad in C.A. No.2867/2012, Dr.\\nSukumar Mukherjee in C.A. No.858/2012 and Dr. Baidyanath Haldar in C.A. 731/2012 with regard\\nto percentage, on the basis of costs imposed in paragraph 196 of the judgment in the earlier round of\\nlitigation is without any basis and further submitted that under the heading â€“ â€˜Individual Liability of\\nDoctorsâ€™ findings as to what was the negligence of the doctors and the appellant AMRI Hospital is\\nnot stated. If the said findings of the National Commission are considered, then it cannot be argued\\nthat the appellant AMRI Hospital should pay the highest compensation. Further, the learned senior\\ncounsel rebutted the submission of the claimant contending that since he had himself claimed\\nspecial damages against the appellant-doctors, the Hospital and Dr. Abani Roy Choudhary in the\\ncomplaint before the National Commission, therefore, he cannot now contend contrary to the same\\nin the appeal before this Court.\\nCIVIL APPEAL NO. 858 OF 2012\\n39. It is the case of the appellant- Dr. Sukumar Mukherjee that the National Commission while\\napportioning the liability of the appellant, has wrongly observed that :\\nIndian Kanoon - http://indiankanoon.org/doc/35346928/ 14\\n\\nPage 15 Text:\\nBalram Prasad vs Kunal Saha & Ors on 24 October, 2013\\nâ€œSupreme Court has primarily found Dr.Sukumar Mukherjee and AMRI hospital\\nguilty of negligence and deficient in service on several counts. Therefore, going by the\\nsaid findings and observations of Supreme Court we consider it appropriate to\\napportion the liability of Dr. Sukumar Mukherjee and AMRI hospital in equal\\nproportion, i.e. each should pay 25% i.e. 38,90,000/- of the awarded amount of\\n1,55,60,000/-.â€\\n40. It is submitted by the learned counsel for the appellant - Dr. Sukumar Mukherjee that scrutiny\\nof the judgment in Malay Kumar Gangulyâ€™s case (supra) will show that at no place did the Honâ€™ble\\nSupreme Court made any observation or recorded any finding that the appellant Dr. Mukherjee and\\nthe Hospital are primarily responsible.Kunal Kaha provided 3 concrete proves to the court which were enough to prove AMRI hospital guilty')\"\"\"\n",
    "prompt = f\"<|system|>\\nYou are a lawyer chatbot.</s>\\n<|user|>\\n Please reply only Yes or no if the query: What are the reasoning give by kunal saha to prove AMRI hospital guilty ? can be answered as per the context {context}</s>\\n<|assistant|>\\n\"\n",
    "st = time.time()\n",
    "\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.2,\n",
    "      echo=True\n",
    ")\n",
    "\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "print(out)\n",
    "print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36f71d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 402 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       0.69 ms /    18 runs   (    0.04 ms per token, 26124.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4493.97 ms /   402 tokens (   11.18 ms per token,    89.45 tokens per second)\n",
      "llama_print_timings:        eval time =    1423.56 ms /    17 runs   (   83.74 ms per token,    11.94 tokens per second)\n",
      "llama_print_timings:       total time =    5926.30 ms /   419 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9307334423065186\n",
      "Tokens: 226\n",
      " \n",
      "\n",
      "Yes. Shah Jahan commissioned the construction of the Taj Mahal.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "The Taj Mahal (/ˌtɑːdʒ məˈhɑːl, ˌtɑːʒ -/ TAHJ mə-HAHL, TAHZH -⁠, Hindi: [taːdʒ ˈmɛɦ(ɛ)l]; lit. 'Crown of the Palace') is an ivory-white marble mausoleum on the right bank of the river Yamuna in Agra, Uttar Pradesh, India. It was commissioned in 1631 by the fifth Mughal emperor, Shah Jahan (r. 1628–1658) to house the tomb of his beloved wife, Mumtaz Mahal; it also houses the tomb of Shah Jahan himself. The tomb is the centrepiece of a 17-hectare (42-acre) complex, which includes a mosque and a guest house, and is set in formal gardens bounded on three sides by a crenellated wall.\n",
    "Construction of the mausoleum was completed in 1648, but work continued on other phases of the project for another five years. The first ceremony held at the mausoleum was an observance by Shah Jahan, on 6 February 1643, of the 12th anniversary of the death of Mumtaz Mahal. The Taj Mahal complex is believed to have been completed in its entirety in 1653 at a cost estimated at the time to be around ₹5 million, which in 2023 would be approximately ₹35 billion (US$77.8 million).\n",
    "\"\"\"\n",
    "\n",
    "query=\"who build taj mahal ?\"\n",
    "\n",
    "prompt_1 = f\"\"\"Pleae reply yes if we can answer the question and no if we cannot :##{query}## using the below context: {context}.Only reply using yes or no.\"\"\"\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "output = llm(\n",
    "      prompt = prompt_1,\n",
    "      max_tokens=1024,\n",
    "    temperature = 0.01,\n",
    "      echo=True \n",
    ")\n",
    "\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt_1,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e1ffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 19 prefix-match hit, remaining 383 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       0.67 ms /    17 runs   (    0.04 ms per token, 25411.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3487.73 ms /   383 tokens (    9.11 ms per token,   109.81 tokens per second)\n",
      "llama_print_timings:        eval time =    1144.73 ms /    16 runs   (   71.55 ms per token,    13.98 tokens per second)\n",
      "llama_print_timings:       total time =    4639.97 ms /   399 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.645523309707642\n",
      "Pleae reply yes if we can answer the question and no if we cannot :##who is the president of India?## using the below context: \n",
      "The Taj Mahal (/ˌtɑːdʒ məˈhɑːl, ˌtɑːʒ -/ TAHJ mə-HAHL, TAHZH -⁠, Hindi: [taːdʒ ˈmɛɦ(ɛ)l]; lit. 'Crown of the Palace') is an ivory-white marble mausoleum on the right bank of the river Yamuna in Agra, Uttar Pradesh, India. It was commissioned in 1631 by the fifth Mughal emperor, Shah Jahan (r. 1628–1658) to house the tomb of his beloved wife, Mumtaz Mahal; it also houses the tomb of Shah Jahan himself. The tomb is the centrepiece of a 17-hectare (42-acre) complex, which includes a mosque and a guest house, and is set in formal gardens bounded on three sides by a crenellated wall.\n",
      "Construction of the mausoleum was completed in 1648, but work continued on other phases of the project for another five years. The first ceremony held at the mausoleum was an observance by Shah Jahan, on 6 February 1643, of the 12th anniversary of the death of Mumtaz Mahal. The Taj Mahal complex is believed to have been completed in its entirety in 1653 at a cost estimated at the time to be around ₹5 million, which in 2023 would be approximately ₹35 billion (US$77.8 million).\n",
      ".Only reply using yes or no.## No. The context does not provide information about the current president of India.\n",
      "Tokens: 230\n",
      " ## No. The context does not provide information about the current president of India.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "The Taj Mahal (/ˌtɑːdʒ məˈhɑːl, ˌtɑːʒ -/ TAHJ mə-HAHL, TAHZH -⁠, Hindi: [taːdʒ ˈmɛɦ(ɛ)l]; lit. 'Crown of the Palace') is an ivory-white marble mausoleum on the right bank of the river Yamuna in Agra, Uttar Pradesh, India. It was commissioned in 1631 by the fifth Mughal emperor, Shah Jahan (r. 1628–1658) to house the tomb of his beloved wife, Mumtaz Mahal; it also houses the tomb of Shah Jahan himself. The tomb is the centrepiece of a 17-hectare (42-acre) complex, which includes a mosque and a guest house, and is set in formal gardens bounded on three sides by a crenellated wall.\n",
    "Construction of the mausoleum was completed in 1648, but work continued on other phases of the project for another five years. The first ceremony held at the mausoleum was an observance by Shah Jahan, on 6 February 1643, of the 12th anniversary of the death of Mumtaz Mahal. The Taj Mahal complex is believed to have been completed in its entirety in 1653 at a cost estimated at the time to be around ₹5 million, which in 2023 would be approximately ₹35 billion (US$77.8 million).\n",
    "\"\"\"\n",
    "\n",
    "query=\"who is the president of India?\"\n",
    "\n",
    "prompt_1 = f\"\"\"Pleae reply yes if we can answer the question and no if we cannot :##{query}## using the below context: {context}.Only reply using yes or no.\"\"\"\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "output = llm(\n",
    "      prompt = prompt_1,\n",
    "      max_tokens=1024,\n",
    "    temperature = 0.01,\n",
    "      echo=True \n",
    ")\n",
    "\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt_1,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e77059a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       0.13 ms /     3 runs   (    0.04 ms per token, 22556.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1355.91 ms /   150 tokens (    9.04 ms per token,   110.63 tokens per second)\n",
      "llama_print_timings:        eval time =     162.65 ms /     2 runs   (   81.32 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =    1519.95 ms /   152 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.387964725494385\n",
      "Tokens: 98\n",
      "  Yes.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"(12, 'The company’s recent marketing campaign focused on highlighting the environmental benefits of its new product line. The advertisements claimed that the products were 100% biodegradable and made from recycled materials. However, an independent investigation revealed that only 60% of the product was biodegradable, and the remaining 40% contained non-recyclable plastics. The public backlash was immediate, and the company faced accusations of false advertising.')\"\"\"\n",
    "prompt=f\"You are a marketing chatbot.</s>Please reply only Yes or no if the query: Can the company’s advertisement claims be considered misleading based on the investigation findings? can be answered as per the context {context}</s>\"\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature = 0.2,\n",
    "      echo=True \n",
    ")\n",
    "\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d375f5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 82 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       1.05 ms /    27 runs   (    0.04 ms per token, 25592.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    1910.50 ms /    27 runs   (   70.76 ms per token,    14.13 tokens per second)\n",
      "llama_print_timings:       total time =    1922.05 ms /    27 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9270257949829102\n",
      "Tokens: 82\n",
      "  Yes, based on the information provided in the context, the new renewable energy source is not ready for large-scale production.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"The context mentions a scientific study on a new type of renewable energy source. However, the study concluded that the energy source is not yet viable for large-scale production due to inefficiencies.\"\"\"\n",
    "query=\" Is the new renewable energy source ready for large-scale production? \"\n",
    "prompt=f\"You are a science chatbot.</s>Please reply only Yes or no if the query:{query}can be answered as per the context {context}</s>\"\n",
    "st = time.time()\n",
    "\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.95,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be6133f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 11 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       5.99 ms /   154 runs   (    0.04 ms per token, 25692.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     620.85 ms /    65 tokens (    9.55 ms per token,   104.70 tokens per second)\n",
      "llama_print_timings:        eval time =   10447.31 ms /   153 runs   (   68.28 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =   11149.74 ms /   218 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.154605388641357\n",
      "Tokens: 172\n",
      " \n",
      "\n",
      "Based on the information provided, it appears that the new renewable energy source discussed in the study is not yet ready for large-scale production due to inefficiencies that were identified in the research. It's important to note that scientific research and development often involve iterative processes, and setbacks or challenges are not uncommon. However, they provide valuable insights and opportunities for improvement. The renewable energy industry is constantly evolving, and advancements in technology and efficiency are driving down costs and increasing the feasibility of various renewable energy sources for large-scale production. It's likely that future research and innovation in this area will help address the inefficiencies and bring the new renewable energy source closer to commercial viability.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"The context mentions a scientific study on a new type of renewable energy source. However, the study concluded that the energy source is not yet viable for large-scale production due to inefficiencies.\"\"\"\n",
    "query=\" Is the new renewable energy source ready for large-scale production? \"\n",
    "prompt=f\"You are a science chatbot.</s>Please reply to the query:{query}as per the context {context}</s>\"\n",
    "st = time.time()\n",
    "\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.95,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0014218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 4 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       1.68 ms /    43 runs   (    0.04 ms per token, 25534.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     456.67 ms /    39 tokens (   11.71 ms per token,    85.40 tokens per second)\n",
      "llama_print_timings:        eval time =    2905.36 ms /    42 runs   (   69.18 ms per token,    14.46 tokens per second)\n",
      "llama_print_timings:       total time =    3382.01 ms /    81 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3874905109405518\n",
      "Tokens: 59\n",
      " \n",
      "\n",
      "Based on the context provided, the novel \"Neuromancer\" by William Gibson does explore the impact of AI on employment and society.\n",
      "\n",
      "Therefore, the answer is: Yes.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"(13, 'The novel depicts a dystopian future where artificial intelligence has taken over most human jobs, leading to widespread unemployment and social unrest. The protagonist, a former software engineer, becomes part of an underground resistance group that aims to overthrow the AI-controlled government. Throughout the story, the author explores themes of technology, ethics, and human resilience.')\"\"\"\n",
    "prompt=\"You are a literature chatbot.</s>Please reply only Yes or no if the query: Does the novel explore the impact of AI on employment and society? can be answered as per the context {context}</s>\"\n",
    "st = time.time()\n",
    "\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.95,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3de76fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 4 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       0.92 ms /    23 runs   (    0.04 ms per token, 24945.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     795.18 ms /    79 tokens (   10.07 ms per token,    99.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1552.67 ms /    22 runs   (   70.58 ms per token,    14.17 tokens per second)\n",
      "llama_print_timings:       total time =    2357.86 ms /   101 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3615589141845703\n",
      "Tokens: 79\n",
      "  Yes. The study indicates that the new renewable energy source is not yet ready for large-scale production.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"The context mentions a scientific study on a new type of renewable energy source. However, the study concluded that the energy source is not yet viable for large-scale production due to inefficiencies.\"\"\"\n",
    "query=\" Is the new renewable energy source ready for large-scale production? \"\n",
    "prompt=f\"You are a science chatbot.</s>Please reply only Yes or no if the query:{query}can be answered as per the context {context}</s>\"\n",
    "st = time.time()\n",
    "\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.2,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7295678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 4 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       1.62 ms /    42 runs   (    0.04 ms per token, 25846.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     907.79 ms /    84 tokens (   10.81 ms per token,    92.53 tokens per second)\n",
      "llama_print_timings:        eval time =    2800.30 ms /    41 runs   (   68.30 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =    3727.15 ms /   125 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7313039302825928\n",
      "Tokens: 99\n",
      "  No, the context does not mention the explorer's favorite food. The focus of the text is on the historical event and its significance, not on the explorer's personal preferences or habits.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"The context describes a historical event in which a famous explorer discovered a new continent in the 15th century. It details the challenges faced during the voyage, the interactions with indigenous peoples, and the subsequent impact on global trade routes.\"\"\"\n",
    "prompt=f\"You are a history chatbot.</s>Please reply only Yes or no if the query: Does the context mention the explorer's favorite food? can be answered as per the context {context}</s>\"\n",
    "st = time.time()\n",
    "\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.95,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe91d920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       0.53 ms /    14 runs   (    0.04 ms per token, 26315.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =     950.77 ms /    14 runs   (   67.91 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =     956.56 ms /    14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9610958099365234\n",
      "Tokens: 77\n",
      "  The context does not mention the explorer's favorite food.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"The context describes a historical event in which a famous explorer discovered a new continent in the 15th century. It details the challenges faced during the voyage, the interactions with indigenous peoples, and the subsequent impact on global trade routes.\"\"\"\n",
    "prompt=f\"You are a history chatbot.</s>Please reply only Yes or no if the query: Does the context mention the explorer's favorite food? can be answered as per the context {context}</s>\"\n",
    "st = time.time()\n",
    "\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.2,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9a534e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 395 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /    30 runs   (    0.04 ms per token, 25862.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3714.74 ms /   395 tokens (    9.40 ms per token,   106.33 tokens per second)\n",
      "llama_print_timings:        eval time =    2007.85 ms /    29 runs   (   69.24 ms per token,    14.44 tokens per second)\n",
      "llama_print_timings:       total time =    5736.36 ms /   424 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.742085933685303\n",
      "Tokens: 229\n",
      " \n",
      "\n",
      "Yes, we can answer the question. The Taj Mahal was built by the fifth Mughal emperor, Shah Jahan.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "The Taj Mahal (/ˌtɑːdʒ məˈhɑːl, ˌtɑːʒ -/ TAHJ mə-HAHL, TAHZH -⁠, Hindi: [taːdʒ ˈmɛɦ(ɛ)l]; lit. 'Crown of the Palace') is an ivory-white marble mausoleum on the right bank of the river Yamuna in Agra, Uttar Pradesh, India. It was commissioned in 1631 by the fifth Mughal emperor, Shah Jahan (r. 1628–1658) to house the tomb of his beloved wife, Mumtaz Mahal; it also houses the tomb of Shah Jahan himself. The tomb is the centrepiece of a 17-hectare (42-acre) complex, which includes a mosque and a guest house, and is set in formal gardens bounded on three sides by a crenellated wall.\n",
    "Construction of the mausoleum was completed in 1648, but work continued on other phases of the project for another five years. The first ceremony held at the mausoleum was an observance by Shah Jahan, on 6 February 1643, of the 12th anniversary of the death of Mumtaz Mahal. The Taj Mahal complex is believed to have been completed in its entirety in 1653 at a cost estimated at the time to be around ₹5 million, which in 2023 would be approximately ₹35 billion (US$77.8 million).\n",
    "\"\"\"\n",
    "\n",
    "query=\"who build taj mahal ?\"\n",
    "\n",
    "prompt_1 = f\"\"\"Pleae reply yes if we can answer the question and no if we cannot :##{query}## using the below context: {context}.\"\"\"\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "output = llm(\n",
    "      prompt = prompt_1,\n",
    "      max_tokens=1024,\n",
    "    temperature = 0.01,\n",
    "      echo=True \n",
    ")\n",
    "\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt_1,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "201c2706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 11 prefix-match hit, remaining 379 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       0.67 ms /    17 runs   (    0.04 ms per token, 25525.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3449.89 ms /   379 tokens (    9.10 ms per token,   109.86 tokens per second)\n",
      "llama_print_timings:        eval time =    1111.84 ms /    16 runs   (   69.49 ms per token,    14.39 tokens per second)\n",
      "llama_print_timings:       total time =    4569.84 ms /   395 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.574877500534058\n",
      "Tokens: 219\n",
      " \n",
      "No, the context does not provide information about the current president of India.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "The Taj Mahal (/ˌtɑːdʒ məˈhɑːl, ˌtɑːʒ -/ TAHJ mə-HAHL, TAHZH -⁠, Hindi: [taːdʒ ˈmɛɦ(ɛ)l]; lit. 'Crown of the Palace') is an ivory-white marble mausoleum on the right bank of the river Yamuna in Agra, Uttar Pradesh, India. It was commissioned in 1631 by the fifth Mughal emperor, Shah Jahan (r. 1628–1658) to house the tomb of his beloved wife, Mumtaz Mahal; it also houses the tomb of Shah Jahan himself. The tomb is the centrepiece of a 17-hectare (42-acre) complex, which includes a mosque and a guest house, and is set in formal gardens bounded on three sides by a crenellated wall.\n",
    "Construction of the mausoleum was completed in 1648, but work continued on other phases of the project for another five years. The first ceremony held at the mausoleum was an observance by Shah Jahan, on 6 February 1643, of the 12th anniversary of the death of Mumtaz Mahal. The Taj Mahal complex is believed to have been completed in its entirety in 1653 at a cost estimated at the time to be around ₹5 million, which in 2023 would be approximately ₹35 billion (US$77.8 million).\n",
    "\"\"\"\n",
    "\n",
    "query=\"who is the president of India?\"\n",
    "\n",
    "prompt_1 = f\"\"\"Pleae reply yes if we can answer the question:##{query}## using the below context: {context}.\"\"\"\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "output = llm(\n",
    "      prompt = prompt_1,\n",
    "      max_tokens=1024,\n",
    "    temperature = 0.01,\n",
    "      echo=True \n",
    ")\n",
    "\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "#print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt_1,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4cdce24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 125 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       1.55 ms /    39 runs   (    0.04 ms per token, 25177.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    2681.60 ms /    39 runs   (   68.76 ms per token,    14.54 tokens per second)\n",
      "llama_print_timings:       total time =    2698.61 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.704274892807007\n",
      "Tokens: 127\n",
      "  Yes.\n",
      "\n",
      "Although there were challenges during the testing phase, the new algorithm successfully reduced delivery times by an average of 20% when implemented, based on the information provided.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"('The team’s project aimed to develop a new algorithm for optimizing delivery routes in urban areas. The algorithm used real-time traffic data and machine learning to predict the fastest routes, reducing delivery times by an average of 20%. During the testing phase, the team encountered challenges with integrating the algorithm into existing GPS systems, but they eventually resolved these issues through collaboration with a software development firm.')\"\"\"\n",
    "prompt=f\"You are a technology chatbot.</s>Please reply only Yes or no if the query: Did the new algorithm successfully reduce delivery times by 20% during testing? can be answered as per the context {context}</s>\"\n",
    "st=time.time()\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.95,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a3eeeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 67 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       4.99 ms /   129 runs   (    0.04 ms per token, 25831.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    8809.41 ms /   129 runs   (   68.29 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =    8876.56 ms /   129 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.880971670150757\n",
      "Tokens: 152\n",
      " \n",
      "\n",
      "No, the query specifically asks about the impact of climate change on marine biodiversity and does not provide enough context for me to provide an answer within the context of creating a detailed report on climate change. However, I can answer that yes, climate change does have an impact on marine biodiversity. Climate change can lead to ocean warming, ocean acidification, sea level rise, and other environmental stressors that can harm marine organisms and ecosystems. These impacts can include changes in distribution, abundance, and diversity of marine species, as well as threats to the structure and function of marine ecosystems.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"The context describes the steps involved in creating a detailed report on climate change, including data collection, analysis, and presentation of findings.\"\"\"\n",
    "prompt=f\"You are a climate change chatbot.</s>Please reply only Yes or no if the query:What is the impact of climate change on marine biodiversity? can be answered as per the context {context}</s>\"\n",
    "\n",
    "st=time.time()\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.95,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d798bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       1.44 ms /    34 runs   (    0.04 ms per token, 23545.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     828.82 ms /    92 tokens (    9.01 ms per token,   111.00 tokens per second)\n",
      "llama_print_timings:        eval time =    2347.22 ms /    33 runs   (   71.13 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =    3191.07 ms /   125 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.196206569671631\n",
      "Tokens: 101\n",
      " \n",
      "\n",
      "No, the query cannot be answered as per the context, as there is no information provided in the context regarding the specific environmental impacts of using the software.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"The context discusses the development of a new software application designed to help businesses manage their inventory more efficiently. It covers features like real-time tracking, automated reordering, and detailed reporting. The document also includes user testimonials about how the software has improved inventory management.\"\"\"\n",
    "prompt=f\"You are a software chatbot.</s>Please reply only Yes or no if the query: What are the environmental impacts of using this software? can be answered as per the context {context}</s>\"\n",
    "\n",
    "st=time.time()\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.95,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5760f706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 4 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       1.17 ms /    28 runs   (    0.04 ms per token, 24013.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     745.59 ms /    82 tokens (    9.09 ms per token,   109.98 tokens per second)\n",
      "llama_print_timings:        eval time =    1893.30 ms /    27 runs   (   70.12 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =    2651.77 ms /   109 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.657007932662964\n",
      "Tokens: 89\n",
      " \n",
      "\n",
      "No, the query specifically asks about the effects on mental health, which is not directly addressed in the context of the study.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\" The context details a comprehensive study on urban transportation infrastructure, including public transit systems, road maintenance, and traffic management. The study includes data on traffic congestion, improvements in transit efficiency, and budget allocations for infrastructure projects.\"\"\"\n",
    "prompt=f\"You are a transportation infrastructure chatbot.</s>Please reply only Yes or no if the query: What are the effects of urban transportation on mental health? can be answered as per the context {context}</s>\"\n",
    "\n",
    "st=time.time()\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.95,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2b9346d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 4 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       1.46 ms /    36 runs   (    0.04 ms per token, 24623.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     733.96 ms /    81 tokens (    9.06 ms per token,   110.36 tokens per second)\n",
      "llama_print_timings:        eval time =    2445.24 ms /    35 runs   (   69.86 ms per token,    14.31 tokens per second)\n",
      "llama_print_timings:       total time =    3195.45 ms /   116 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1997294425964355\n",
      "Tokens: 96\n",
      " \n",
      "\n",
      "No, the context does not provide sufficient information to answer whether or how modern social media trends affect consumer behavior. That topic falls outside the scope of computer technology history.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"The context provides a historical overview of the evolution of computer technology from the 1950s to the present day. It includes information on key developments, influential figures, and major technological milestones in the field of computing.\"\"\"\n",
    "prompt=f\"You are a technology history chatbot.</s>Please reply only Yes or no if the query: How do modern social media trends affect consumer behavior? can be answered as per the context {context}</s>\"\n",
    "\n",
    "st=time.time()\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.95,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7950e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 4 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     3 runs   (    0.05 ms per token, 22222.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1063.83 ms /   116 tokens (    9.17 ms per token,   109.04 tokens per second)\n",
      "llama_print_timings:        eval time =     154.17 ms /     2 runs   (   77.08 ms per token,    12.97 tokens per second)\n",
      "llama_print_timings:       total time =    1219.56 ms /   118 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.224496603012085\n",
      "Tokens: 93\n",
      "  Yes.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"('The athlete’s rigorous training regimen included daily sessions of strength training, cardio, and flexibility exercises. Despite a minor injury early in the season, the athlete maintained a strict recovery plan and returned to peak performance in time for the championship. The dedication paid off, as the athlete secured a gold medal in the final event, setting a new personal record.')\"\"\"\n",
    "prompt=f\"You are a sports chatbot.</s>Please reply only Yes or no if the query: Did the athlete achieve a new personal record during the championship event? can be answered as per the context {context}</s>\"\n",
    "\n",
    "st=time.time()\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.95,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c0d6469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 119 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       0.11 ms /     3 runs   (    0.04 ms per token, 26315.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =     213.37 ms /     3 runs   (   71.12 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =     215.23 ms /     3 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22036075592041016\n",
      "Tokens: 93\n",
      "  Yes.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"('The athlete’s rigorous training regimen included daily sessions of strength training, cardio, and flexibility exercises. Despite a minor injury early in the season, the athlete maintained a strict recovery plan and returned to peak performance in time for the championship. The dedication paid off, as the athlete secured a gold medal in the final event, setting a new personal record.')\"\"\"\n",
    "prompt=f\"You are a sports chatbot.</s>Please reply only Yes or no if the query: Did the athlete achieve a new personal record during the championship event? can be answered as per the context {context}</s>\"\n",
    "\n",
    "st=time.time()\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.2,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "512cead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 4 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       1.69 ms /    43 runs   (    0.04 ms per token, 25504.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     775.30 ms /    81 tokens (    9.57 ms per token,   104.48 tokens per second)\n",
      "llama_print_timings:        eval time =    2872.87 ms /    42 runs   (   68.40 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    3667.72 ms /   123 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.672211170196533\n",
      "Tokens: 103\n",
      " \n",
      "\n",
      "No, the context does not provide sufficient information to answer the query about modern social media trends and their effect on consumer behavior. This topic falls outside the scope of computer technology history covered in the context.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"The context provides a historical overview of the evolution of computer technology from the 1950s to the present day. It includes information on key developments, influential figures, and major technological milestones in the field of computing.\"\"\"\n",
    "prompt=f\"You are a technology history chatbot.</s>Please reply only Yes or no if the query: How do modern social media trends affect consumer behavior? can be answered as per the context {context}</s>\"\n",
    "\n",
    "st=time.time()\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.2,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e328c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 84 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    5037.53 ms\n",
      "llama_print_timings:      sample time =       1.74 ms /    43 runs   (    0.04 ms per token, 24712.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    2912.73 ms /    43 runs   (   67.74 ms per token,    14.76 tokens per second)\n",
      "llama_print_timings:       total time =    2932.08 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.937185287475586\n",
      "Tokens: 103\n",
      " \n",
      "\n",
      "No, the context does not provide sufficient information to answer the query about modern social media trends and their impact on consumer behavior. This topic falls outside the scope of computer technology history covered in the context.\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\"The context provides a historical overview of the evolution of computer technology from the 1950s to the present day. It includes information on key developments, influential figures, and major technological milestones in the field of computing.\"\"\"\n",
    "prompt=f\"You are a technology history chatbot.</s>Please reply only Yes or no if the query: How do modern social media trends affect consumer behavior? can be answered as per the context {context}</s>\"\n",
    "\n",
    "st=time.time()\n",
    "output = llm(\n",
    "      prompt = prompt,\n",
    "      max_tokens=1024,\n",
    "    temperature=0.01,\n",
    "      echo=True \n",
    ")\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "out = output['choices'][0][\"text\"]\n",
    "# print(out)\n",
    "# print(\"*\"*50)\n",
    "print(\"Tokens:\",len(out.split()))\n",
    "out = out.replace(prompt,\" \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d47469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad71fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317a183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab3d5ed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "devtmpfs        125G     0  125G   0% /dev\r\n",
      "tmpfs           125G  8.0K  125G   1% /dev/shm\r\n",
      "tmpfs           125G  904K  125G   1% /run\r\n",
      "tmpfs           125G     0  125G   0% /sys/fs/cgroup\r\n",
      "/dev/nvme0n1p1  135G   93G   43G  69% /\r\n",
      "tmpfs            25G     0   25G   0% /run/user/0\r\n",
      "/dev/nvme2n1    197G   54G  136G  29% /home/ec2-user/SageMaker\r\n",
      "tmpfs            25G     0   25G   0% /run/user/1002\r\n",
      "tmpfs            25G     0   25G   0% /run/user/1001\r\n",
      "tmpfs            25G     0   25G   0% /run/user/1000\r\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0dabea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep  7 18:50:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   24C    P8               9W / 300W |      0MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a01be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
